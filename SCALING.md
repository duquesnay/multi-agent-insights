# Performance Thresholds

Simple guide: **When to worry about performance**

## Current Scale

- **Sessions**: 7
- **Delegations**: ~264
- **Performance**: Excellent (~2s extraction, 50MB memory)

## When Performance Becomes a Problem

### âœ… You're Fine (Current State)
- < 100 sessions â†’ Everything works great
- < 500 sessions â†’ No changes needed

### âš ï¸ Watch Performance (500-1000 sessions)
- Extraction: May reach ~10-15s
- Memory: ~200-500MB
- **Action**: Monitor, no changes needed yet

### ðŸ”´ Optimization Required (1000+ sessions)
- Extraction: >30s (becomes slow)
- Memory: >1GB (may cause issues)

**At this scale, contact the team for optimization support.**

## Quick Performance Check

```bash
# Time the extraction
time python -m tools.pipeline.extract_all_sessions

# If it takes >5s with <100 sessions, something's wrong
```

## What NOT to Do

âŒ Don't optimize prematurely
âŒ Don't add complex caching for <1000 sessions
âŒ Don't switch to stream processing until you need it

## What TO Do

âœ… Keep it simple (current approach works)
âœ… Monitor extraction time as data grows
âœ… Contact the team when you hit 1000+ sessions

---

**Bottom line**: The current implementation is well-designed and will handle 10x-100x growth without changes.
